{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281c0c3-58b4-4bf2-bdd2-529f4ee75d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c985e63-545b-4a53-823b-ddfdaffd432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"kidney_disease.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f00e0-1fd3-4085-8986-785071830252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1190)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221ecbc-8f80-4c62-85f9-6db41e0064e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['classification'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b00450-7a4a-4c9f-945d-d30c48209d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e383d2-a0a0-4eeb-9461-d2837680205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id', axis=1 , inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e07d58-bf86-4849-bdbf-0738758ed46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc05713-cb27-4958-958c-e2310978be4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['age', 'blood_pressure', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',\n",
    "              'pus_cell_clumps', 'bacteria', 'blood_glucose_random', 'blood_urea', 'serum_creatinine', 'sodium',\n",
    "              'potassium', 'haemoglobin', 'packed_cell_volume', 'white_blood_cell_count', 'red_blood_cell_count',\n",
    "              'hypertension', 'diabetes_mellitus', 'coronary_artery_disease', 'appetite', 'peda_edema',\n",
    "              'aanemia', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9473c9-791d-4ca2-9658-5e0a0a8167f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3424f07-7f3f-4a51-8468-caf874baa710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac582a-b1fd-488c-901f-a8ba80b169bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b979271-44a6-4c6c-a844-a6a1a663f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['packed_cell_volume'] = pd.to_numeric(df['packed_cell_volume'], errors='coerce')\n",
    "df['white_blood_cell_count'] = pd.to_numeric(df['white_blood_cell_count'], errors='coerce')\n",
    "df['red_blood_cell_count'] = pd.to_numeric(df['red_blood_cell_count'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc98643-cd90-4eed-b55e-78e860582254",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db473fa-3fd0-4f5d-8362-e81a8d75ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d570208-21c4-459c-bb91-3cb117a48c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "num_cols = [col for col in df.columns if df[col].dtype != 'object']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f0150-603f-423f-85f4-011b2a0ed10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b50c5da-129c-4c25-ba8c-4940308e3bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c04353a-dcee-47ea-918f-5afe3f89510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    print(f\"{col} has {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380b34d-8c71-4b30-98c0-0518b25f8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diabetes_mellitus'].replace(to_replace = {'\\tno':'no', '\\tyes': 'yes', ' yes':'yes'}, inplace=True)\n",
    "df['coronary_artery_disease'] = df['coronary_artery_disease'].replace(to_replace = '\\tno', value = 'no')\n",
    "df['class'] = df['class'].replace(to_replace={'ckd\\t':'ckd', 'notckd': 'not ckd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3b315-5b49-4de3-9443-71c53c0ad8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['diabetes_mellitus', 'coronary_artery_disease', 'class']\n",
    "for col in cols:\n",
    "    print(f\"{col} has {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec262cc9-965b-47b6-8d1c-097db6e27b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['class'].map({'ckd':0, 'not ckd': 1})\n",
    "df['class'] = pd.to_numeric(df['class'], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc467dba-b5ca-4a8a-9e64-53e507bb4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['diabetes_mellitus', 'coronary_artery_disease', 'class']\n",
    "for col in cols:\n",
    "    print(f\"{col} has {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91197d-6295-4f03-8189-dae2d2bb1b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 15))\n",
    "plotnumber = 1\n",
    "\n",
    "for column in num_cols:\n",
    "    if plotnumber <= 14:\n",
    "        ax = plt.subplot(3, 5, plotnumber)\n",
    "        sns.distplot(df[column])\n",
    "        plt.xlabel(column)\n",
    "        \n",
    "    plotnumber += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf7bbf-8e5a-44a7-aba4-ca515937581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (20, 15))\n",
    "plotnumber = 1\n",
    "\n",
    "for column in cat_cols:\n",
    "    if plotnumber <= 14:\n",
    "        ax = plt.subplot(3, 5, plotnumber)\n",
    "        import seaborn as sns\n",
    "        sns.barplot(x=df[column].value_counts().index, y=df[column].value_counts(), palette='rocket')\n",
    "        plt.xlabel(column)\n",
    "        \n",
    "    plotnumber += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a0d4e-392f-452e-a21d-01572da788b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert non-numeric columns to numeric using label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a copy of the DataFrame to preserve the original\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Iterate through each column\n",
    "for col in df_encoded.columns:\n",
    "    # Check if the column contains non-numeric data\n",
    "    if df_encoded[col].dtype == 'object':\n",
    "        # Use label encoding to convert the non-numeric data to numeric\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df_encoded.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eea7f0-b715-476b-aeab-9e906ced5f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736f322-cce8-4ff0-b824-09e367c0f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kde(col):\n",
    "    grid = sns.FacetGrid(df, hue=\"class\", height = 6, aspect=2)\n",
    "    grid.map(sns.kdeplot, col)\n",
    "    grid.add_legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34971969-ae88-4030-9873-05c0e722b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde('red_blood_cell_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d3e3e-1048-4a5f-bfcf-60cf192c9f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde('white_blood_cell_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff5272c-15a7-42e2-a766-d142a4a5528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdce608-5b01-44b8-845e-0a216864286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing value\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03918b-6161-4d93-9e01-01f464d9c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a5441-4640-4d02-9e30-93da107a9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cat_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b12d6f-ab5c-4c05-93a2-da151bcdc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd36e3-84c8-4507-af12-70fca8d6025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two method\n",
    "# radom sampling->higer null value\n",
    "# mean/mode-> lower null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c71f52-08e3-4482-91ef-666e75fb0679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(feature):\n",
    "    random_sample = df[feature].dropna().sample(df[feature].isna().sum())\n",
    "    random_sample.index = df[df[feature].isnull()].index\n",
    "    df.loc[df[feature].isnull(), feature] = random_sample\n",
    "\n",
    "def impute_mode(feature):\n",
    "    mode = df[feature].mode()[0]\n",
    "    df[feature] = df[feature].fillna(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fc652-703c-42c4-a816-4732b5dbb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sampling for numerical value\n",
    "for col in num_cols:\n",
    "    random_sampling(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a19fc-d5c7-48af-a835-bd81051974fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_cols].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e018e1-b096-4707-9e51-35db903f6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sampling('red_blood_cells')\n",
    "random_sampling('pus_cell')\n",
    "\n",
    "for col in cat_cols:\n",
    "    impute_mode(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7dffc-cbb7-45e1-903f-c90982dc0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cat_cols].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72cb9b-f076-411d-8a88-f0429e283339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f5c8e-bf4c-4cc4-b8ff-91a923398ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    print(f\"{col} has {df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a56895-73cf-4d94-9304-5fac5f4aed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in cat_cols:\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6eaccb-b357-4552-8546-fac15b914e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e120a8b7-a3be-4052-9503-28a5eb5df409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf18051-b06b-4210-9968-48a1ae06ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis = 1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ddd227-2940-4ead-9413-75d810d4944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9dd9d-add4-475c-b27c-341253a627e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535837a-faf7-404e-b494-89bdc2b793ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test, y_train, y_test =  train_test_split(X,y, test_size = 0.4, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8550ce",
   "metadata": {},
   "source": [
    "# KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13967b-2686-4e26-aa7b-a46336df1dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "knn_acc = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"Training Accuracy of KNN is {accuracy_score(y_train, knn.predict(X_train))}\")\n",
    "print(f\"Testing Accuracy of KNN is {accuracy_score(y_test, y_pred_knn)}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "print(f\"Confusion Matrix of KNN is \\n {cm_knn}\\n\")\n",
    "\n",
    "# Classification Report\n",
    "print(f\"Classification Report of KNN is \\n{classification_report(y_test, y_pred_knn)}\")\n",
    "\n",
    "# Extract TP, TN, FP, FN\n",
    "TN, FP, FN, TP = cm_knn.ravel()\n",
    "\n",
    "# Print TP, TN, FP, FN\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "TPR = TP / (TP + FN)  # True Positive Rate\n",
    "FNR = FN / (TP + FN)  # False Negative Rate\n",
    "TNR = TN / (TN + FP)  # True Negative Rate\n",
    "FPR = FP / (TN + FP)  # False Positive Rate\n",
    "\n",
    "print(f\"True Positive Rate (TPR): {TPR}\")\n",
    "print(f\"False Negative Rate (FNR): {FNR}\")\n",
    "print(f\"True Negative Rate (TNR): {TNR}\")\n",
    "print(f\"False Positive Rate (FPR): {FPR}\")\n",
    "\n",
    "# ROC and AUC\n",
    "y_prob_knn = knn.predict_proba(X_test)[:, 1]  \n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob_knn)\n",
    "knn_auc = roc_auc_score(y_test, y_prob_knn)\n",
    "print(f\"AUC Score of KNN: {knn_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6302181-a20b-49ff-a5ed-d0107fd96c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, knn.predict(X_test))\n",
    "\n",
    "# Visualize confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix for KNN')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9de7d9",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364f536-d0ae-4527-aafd-bc965e0702ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Initialize and fit the Random Forest model\n",
    "rand_clf = RandomForestClassifier(\n",
    "    criterion=\"gini\", \n",
    "    max_depth=10, \n",
    "    max_features=\"sqrt\", \n",
    "    min_samples_leaf=1, \n",
    "    min_samples_split=7, \n",
    "    n_estimators=400\n",
    ")\n",
    "rand_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred = rand_clf.predict(X_train)\n",
    "y_test_pred = rand_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Training Accuracy of Random Forest is {train_accuracy}\")\n",
    "print(f\"Testing Accuracy of Random Forest is {test_accuracy}\")\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "class_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Confusion Matrix of Random Forest is \\n{conf_matrix}\\n\")\n",
    "print(f\"Classification Report of Random Forest is \\n{class_report}\")\n",
    "\n",
    "# Extract True Positives, True Negatives, False Positives, False Negatives\n",
    "TP = conf_matrix[1, 1]\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "\n",
    "# Print TP, TN, FP, FN\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "\n",
    "# Calculate TPR, FNR, TNR, FPR\n",
    "TPR = TP / (TP + FN)\n",
    "FNR = FN / (TP + FN)\n",
    "TNR = TN / (TN + FP)\n",
    "FPR = FP / (TN + FP)\n",
    "\n",
    "# Print TPR, FNR, TNR, FPR\n",
    "print(f\"True Positive Rate (TPR): {TPR}\")\n",
    "print(f\"False Negative Rate (FNR): {FNR}\")\n",
    "print(f\"True Negative Rate (TNR): {TNR}\")\n",
    "print(f\"False Positive Rate (FPR): {FPR}\")\n",
    "\n",
    "# Calculate AUC Score\n",
    "if len(set(y_test)) == 2:  # Ensure binary classification for AUC calculation\n",
    "    auc_score = roc_auc_score(y_test, rand_clf.predict_proba(X_test)[:, 1])\n",
    "    print(f\"AUC Score: {auc_score}\")\n",
    "else:\n",
    "    print(\"AUC Score: Not applicable for non-binary classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c75cb4-f73c-44ac-aad9-9409d3ddc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Calculate confusion matrix for Random Forest\n",
    "rf_cm = confusion_matrix(y_test, rand_clf.predict(X_test))\n",
    "\n",
    "# Visualize confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix for Random Forest Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5485d",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f30d3b-daea-4886-b149-aa09a8c7eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Initialize and fit the Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Training Accuracy of Logistic Regression is {train_accuracy}\")\n",
    "print(f\"Testing Accuracy of Logistic Regression is {test_accuracy}\")\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "class_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Confusion Matrix of Logistic Regression is \\n{conf_matrix}\\n\")\n",
    "print(f\"Classification Report of Logistic Regression is \\n{class_report}\")\n",
    "\n",
    "# Extract True Positives, True Negatives, False Positives, False Negatives\n",
    "TP = conf_matrix[1, 1]\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "\n",
    "# Print TP, TN, FP, FN\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "\n",
    "# Calculate TPR, FNR, TNR, FPR\n",
    "TPR = TP / (TP + FN)\n",
    "FNR = FN / (TP + FN)\n",
    "TNR = TN / (TN + FP)\n",
    "FPR = FP / (TN + FP)\n",
    "\n",
    "# Print TPR, FNR, TNR, FPR\n",
    "print(f\"True Positive Rate (TPR): {TPR}\")\n",
    "print(f\"False Negative Rate (FNR): {FNR}\")\n",
    "print(f\"True Negative Rate (TNR): {TNR}\")\n",
    "print(f\"False Positive Rate (FPR): {FPR}\")\n",
    "\n",
    "# Calculate AUC Score\n",
    "if len(set(y_test)) == 2:  # Ensure binary classification for AUC calculation\n",
    "    auc_score = roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1])\n",
    "    print(f\"AUC Score: {auc_score}\")\n",
    "else:\n",
    "    print(\"AUC Score: Not applicable for non-binary classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d406cd3-7068-4498-807b-92ccd1da7f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate confusion matrix for Logistic Regression\n",
    "lr_cm = confusion_matrix(y_test, lr.predict(X_test))\n",
    "\n",
    "# Visualize confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f66b57",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf92ff-09dc-4a4c-90fe-e1973bea3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# Import necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm = SVC(probability=True)\n",
    "\n",
    "# Define the parameter grid\n",
    "parameter_grid = {\n",
    "    'gamma': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'C': [0.01, 0.05, 0.1, 0.5, 1, 10, 15, 20]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(svm, parameter_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Train the SVC model with the best parameters\n",
    "svm = SVC(gamma=grid_search.best_params_['gamma'], C=grid_search.best_params_['C'], probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred = svm.predict(X_train)\n",
    "y_test_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Training Accuracy of SVC is {train_accuracy}\")\n",
    "print(f\"Testing Accuracy of SVC is {test_accuracy}\")\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "class_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Confusion Matrix of SVC is \\n{conf_matrix}\\n\")\n",
    "print(f\"Classification Report of SVC is \\n{class_report}\")\n",
    "\n",
    "# Extract True Positives, True Negatives, False Positives, False Negatives\n",
    "TP = conf_matrix[1, 1]\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "\n",
    "# Print TP, TN, FP, FN\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "\n",
    "# Calculate TPR, FNR, TNR, FPR\n",
    "TPR = TP / (TP + FN)\n",
    "FNR = FN / (TP + FN)\n",
    "TNR = TN / (TN + FP)\n",
    "FPR = FP / (TN + FP)\n",
    "\n",
    "# Print TPR, FNR, TNR, FPR\n",
    "print(f\"True Positive Rate (TPR): {TPR}\")\n",
    "print(f\"False Negative Rate (FNR): {FNR}\")\n",
    "print(f\"True Negative Rate (TNR): {TNR}\")\n",
    "print(f\"False Positive Rate (FPR): {FPR}\")\n",
    "\n",
    "# Calculate AUC Score\n",
    "if len(set(y_test)) == 2:  # Ensure binary classification for AUC calculation\n",
    "    auc_score = roc_auc_score(y_test, svm.predict_proba(X_test)[:, 1])\n",
    "    print(f\"AUC Score: {auc_score}\")\n",
    "else:\n",
    "    print(\"AUC Score: Not applicable for non-binary classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460b9b8-fba3-4c0d-acbb-7d942e787332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)  # Increase font scale for better readability\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlGnBu', cbar=False, annot_kws={\"size\": 14}, linewidths=.5)\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.xlabel('Predicted Label', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791fa862",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize and fit the Decision Tree model\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred = dtc.predict(X_train)\n",
    "y_test_pred = dtc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Training Accuracy of Decision Tree Classifier is {train_accuracy}\")\n",
    "print(f\"Testing Accuracy of Decision Tree Classifier is {test_accuracy}\")\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "class_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Confusion Matrix of Decision Tree Classifier is \\n{conf_matrix}\\n\")\n",
    "print(f\"Classification Report of Decision Tree Classifier is \\n{class_report}\")\n",
    "\n",
    "# Extract True Positives, True Negatives, False Positives, False Negatives\n",
    "TP = conf_matrix[1, 1]\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "\n",
    "# Print TP, TN, FP, FN\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "\n",
    "# Calculate TPR, FNR, TNR, FPR\n",
    "TPR = TP / (TP + FN)\n",
    "FNR = FN / (TP + FN)\n",
    "TNR = TN / (TN + FP)\n",
    "FPR = FP / (TN + FP)\n",
    "\n",
    "# Print TPR, FNR, TNR, FPR\n",
    "print(f\"True Positive Rate (TPR): {TPR}\")\n",
    "print(f\"False Negative Rate (FNR): {FNR}\")\n",
    "print(f\"True Negative Rate (TNR): {TNR}\")\n",
    "print(f\"False Positive Rate (FPR): {FPR}\")\n",
    "\n",
    "# Calculate AUC Score\n",
    "if len(set(y_test)) == 2:  # Ensure binary classification for AUC calculation\n",
    "    auc_score = roc_auc_score(y_test, dtc.predict_proba(X_test)[:, 1])\n",
    "    print(f\"AUC Score: {auc_score}\")\n",
    "else:\n",
    "    print(\"AUC Score: Not applicable for non-binary classification\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize and fit the Decision Tree model\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = dtc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Testing Accuracy of Decision Tree Classifier is {test_accuracy}\")\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(f\"Confusion Matrix of Decision Tree Classifier is \\n{conf_matrix}\\n\")\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)  # Increase font scale for better readability\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlGnBu', cbar=False, annot_kws={\"size\": 14}, linewidths=.5)\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.xlabel('Predicted Label', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Binarize the output (replace with actual class labels)\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])  \n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Define models and their labels\n",
    "models = [\n",
    "    {'label': 'LR', 'model': lr},\n",
    "    {'label': 'SVM', 'model': svm},\n",
    "    {'label': 'KNN', 'model': knn},\n",
    "    {'label': 'RF', 'model': rand_clf},\n",
    "    {'label': 'DT', 'model': dtc},\n",
    "    \n",
    "]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Loop through each model\n",
    "for m in models:\n",
    "    model = OneVsRestClassifier(m['model'])\n",
    "    model.fit(X_train, label_binarize(y_train, classes=[0, 1, 2, 3]))  # Binarize y_train\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = metrics.roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(y_test_bin.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # Plot micro-average ROC curve\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='%s - ROC (area = %0.2f)' % (m['label'], roc_auc[\"micro\"]))\n",
    "\n",
    "# Plot the diagonal\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "\n",
    "# Set plot limits and labels\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity (False Positive Rate)', fontsize=12)\n",
    "plt.ylabel('Sensitivity (True Positive Rate)', fontsize=12)\n",
    "plt.title('ROC - Kidney Disease Prediction', fontsize=12)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"roc_kidney.jpeg\", format='jpeg', dpi=400, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c997091",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"kidney_disease.csv\")\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "df_numeric.fillna(df_numeric.mean(), inplace=True)\n",
    "df_categorical = df.select_dtypes(include=['object'])\n",
    "le = LabelEncoder()\n",
    "df_categorical_encoded = df_categorical.apply(le.fit_transform)\n",
    "df_processed = pd.concat([df_numeric, df_categorical_encoded], axis=1)\n",
    "\n",
    "# Split into features and target\n",
    "X = df_processed.drop(columns=['classification'])\n",
    "y = df_processed['classification']\n",
    "\n",
    "# 3. Split Data into Train and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "# 4. Initialize Base Models\n",
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# 5. Train Base Models\n",
    "rf.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'rf': rf,\n",
    "    'knn': knn,\n",
    "    'dt': dt\n",
    "}\n",
    "\n",
    "# Create a voting classifier using all three classifiers with soft voting\n",
    "combo = ('rf', 'knn', 'dt')\n",
    "models = [classifiers[clf_name] for clf_name in combo]\n",
    "voting_clf = VotingClassifier(estimators=[(clf_name, clf) for clf_name, clf in zip(combo, models)], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1_score = report['weighted avg']['f1-score']\n",
    "\n",
    "# Calculate confusion matrix and derive metrics\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Initialize metrics\n",
    "tpr = fnr = tnr = fpr = auc_score = None\n",
    "\n",
    "if cm.shape == (2, 2):  # Binary classification case\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    tpr = tp / (tp + fn)  # True Positive Rate\n",
    "    fnr = fn / (tp + fn)  # False Negative Rate\n",
    "    tnr = tn / (tn + fp)  # True Negative Rate\n",
    "    fpr = fp / (tn + fp)  # False Positive Rate\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    y_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_prob)\n",
    "else:  # Multiclass classification case\n",
    "    auc_score = roc_auc_score(y_test, voting_clf.predict_proba(X_test), multi_class='ovr')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Combination: {combo}, Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1_score}\")\n",
    "print(f\"AUC Score: {auc_score}\")\n",
    "\n",
    "\n",
    "train_accuracy = dt.score(X_train, y_train)\n",
    "test_accuracy = dt.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy of ensemble Classifier is {train_accuracy}\")\n",
    "print(f\"Testing Accuracy of ensemble Classifier is {test_accuracy}\")\n",
    "\n",
    "# Confusion Matrix and Classification Report for Decision Tree\n",
    "conf_matrix = confusion_matrix(y_test, dt.predict(X_test))\n",
    "class_report = classification_report(y_test, dt.predict(X_test))\n",
    "print(\"Classification Report for Decision Tree:\\n\", class_report)\n",
    "\n",
    "# Calculate TP, TN, FP, FN\n",
    "if conf_matrix.shape == (2, 2):\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    # Calculate TPR, FNR, TNR, FPR\n",
    "    TPR = tp / (tp + fn)\n",
    "    FNR = fn / (tp + fn)\n",
    "    TNR = tn / (tn + fp)\n",
    "    FPR = fp / (tn + fp)\n",
    "\n",
    "    # Print TP, TN, FP, FN\n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\")\n",
    "\n",
    "    # Print TPR, FNR, TNR, FPR\n",
    "    print(f\"True Positive Rate (TPR): {TPR}\")\n",
    "    print(f\"False Negative Rate (FNR): {FNR}\")\n",
    "    print(f\"True Negative Rate (TNR): {TNR}\")\n",
    "    print(f\"False Positive Rate (FPR): {FPR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Binarize the output (replace with actual class labels)\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])  \n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Define models and their labels\n",
    "models = [\n",
    "    {'label': 'LR', 'model': lr},\n",
    "    {'label': 'SVM', 'model': svm},\n",
    "    {'label': 'KNN', 'model': knn},\n",
    "    {'label': 'RF', 'model': rand_clf},\n",
    "    {'label': 'DT', 'model': dtc},\n",
    "    {'label': 'Ensemble', 'model': voting_clf}\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Loop through each model\n",
    "for m in models:\n",
    "    model = OneVsRestClassifier(m['model'])\n",
    "    model.fit(X_train, label_binarize(y_train, classes=[0, 1, 2, 3]))  # Binarize y_train\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = metrics.roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(y_test_bin.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # Plot micro-average ROC curve\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='%s - ROC (area = %0.2f)' % (m['label'], roc_auc[\"micro\"]))\n",
    "\n",
    "# Plot the diagonal\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "\n",
    "# Set plot limits and labels\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity (False Positive Rate)', fontsize=12)\n",
    "plt.ylabel('Sensitivity (True Positive Rate)', fontsize=12)\n",
    "plt.title('ROC - Kidney Disease Prediction', fontsize=12)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"roc_kidney.jpeg\", format='jpeg', dpi=400, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23e3a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Random Forest': {'precision': 0.98, 'recall': 0.98, 'f1-score': 0.98, 'accuracy': 0.98},\n",
    "    'KNN': {'precision': 0.79, 'recall': 0.79, 'f1-score': 0.79, 'accuracy': 0.798},\n",
    "    'SVC': {'precision': 0.90, 'recall': 0.90, 'f1-score': 0.90, 'accuracy': 0.90},\n",
    "    'Logistic Regression': {'precision': 0.91, 'recall': 0.90, 'f1-score': 0.90, 'accuracy': 0.90},\n",
    "    'Decision Tree': {'precision': 0.97, 'recall': 0.97, 'f1-score': 0.97, 'accuracy': 0.98},\n",
    "    'Ensemble': {'precision': 0.995, 'recall': 0.995, 'f1-score':0.995 , 'accuracy': 0.995}\n",
    "}\n",
    "\n",
    "# Create a bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = range(len(classifiers))\n",
    "classifier_names = list(classifiers.keys())\n",
    "precision = [classifiers[name]['precision'] for name in classifier_names]\n",
    "recall = [classifiers[name]['recall'] for name in classifier_names]\n",
    "f1_score = [classifiers[name]['f1-score'] for name in classifier_names]\n",
    "accuracy = [classifiers[name]['accuracy'] for name in classifier_names]\n",
    "\n",
    "bar_width = 0.15\n",
    "index = x\n",
    "ax.bar(index, precision, bar_width, label='Precision')\n",
    "index = [i + bar_width for i in index]\n",
    "ax.bar(index, recall, bar_width, label='Recall')\n",
    "index = [i + bar_width for i in index]\n",
    "ax.bar(index, f1_score, bar_width, label='F1-Score')\n",
    "index = [i + bar_width for i in index]\n",
    "ax.bar(index, accuracy, bar_width, label='Accuracy')\n",
    "\n",
    "ax.set_xlabel('Classifiers', fontsize=12)\n",
    "ax.set_xticks([i + 1.5 * bar_width for i in x])\n",
    "ax.set_xticklabels(classifier_names, rotation=45, ha='right')\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Performance Comparison of Classifiers', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884c6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
